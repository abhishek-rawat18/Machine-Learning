{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the program is to take  3 inputs namely the document name,size of dataset and frequency threshold (fraction of documents above which the set of words is deemed frequent) in percentage and return the output of itemsets which appears in atleast F percentage of documents and is of size K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas pacakge is used to handle dataframe\n",
    "Efficient-apriori to perform market basket analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_apriori import apriori\n",
    "from efficient_apriori import itemsets_from_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=[\"doc\",\"K\",\"F\",\"time\",\"no of entries\",\"list\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a data frame named data with document name, word collection size K, frequency F, time taken, the number of entries, and the list of K-itemsets.\n",
    "\n",
    "So, in this problem, the items are words and transactions/baskets are the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemset(doc,K,F):\n",
    "    \n",
    "    global data\n",
    "    start=time.process_time()\n",
    "    \n",
    "    df=pd.read_csv(\"docword.\"+doc+\".txt\",sep=\" \",names=[\"docid\",\"wordid\",\"countw\"]) \n",
    "    #df stores the docword file\n",
    "    df=df[3:]\n",
    "    #we don't need the three header lines of docword\n",
    "    \n",
    "    dfv=pd.read_csv(\"vocab.\"+doc+\".txt\",sep=\" \",names=[\"word\"],na_filter=False)\n",
    "    #dfv stores the vocab file\n",
    "    dfv.insert(0,'wordid',range(1,1+len(dfv)))\n",
    "                   \n",
    "    '''The vocab file has all the words present in the document \n",
    "    and the word id corresponding to the word is the row in which the word is present.\n",
    "    As the vocab file does not have a word_id column specifically, so we have created it.'''\n",
    "    \n",
    "    '''While reading the vocab.txt ,na_filter=False is used because some data sets \n",
    "    contain the word \"null\" which is treated as NULL by python. \n",
    "    To avoid this, na_filter is used which ensures that if a word null exists,\n",
    "    it is treated as a string “null”.'''              \n",
    "                   \n",
    "    df=(df.merge(dfv,left_on='wordid',right_on='wordid').reindex(columns=[\"docid\",\"word\",\"countw\"]))\n",
    "    #The vocab dataframe and docid dataframe are merged using the column docid                 \n",
    "\n",
    "    df=df.drop('countw', axis=1)\n",
    "    #This column is not of our use presently, so we drop it\n",
    "                   \n",
    "    df=df.groupby('docid').word.apply(list)\n",
    "    '''We then group all the words occurring in the document and save them as lists.\n",
    "    Each row  in the data frame is docid followed by list of words contained in that document. '''\n",
    "                   \n",
    "    dfl=df.tolist() \n",
    "    #This is then converted to a list of lists as itemset_from_transaction takes input as list of lists.                \n",
    "                   \n",
    "    itemsets=itemsets_from_transactions(dfl, min_support=F/100,max_length=K)\n",
    "    '''Itemsets_from_transactions takes parameters list of lists and min_support and max_length and \n",
    "    runs apriori algorithm on the list and returns the all the items with min support F/100 and all itemsets of size upto K.\n",
    "    The output is a dictionary of dictionaries. The keys of the first dictionary are the lengths of the itemsets.\n",
    "    The values of the dictionary is the dictionary of items.\n",
    "    The keys of the inner dictionary are the itemsets and value is the number of transactions it appears in. '''               \n",
    "    \n",
    "    if len(itemsets[0].keys())==K:\n",
    "      x={\"doc\":doc,\"K\":K,\"F\":F,\"time\":time.clock()-start,\"no of entries\":len(list(itemsets[0][K].keys())),\"list\":list(itemsets[0][K].keys())}\n",
    "    else:\n",
    "      x={\"doc\":doc,\"K\":K,\"F\":F,\"time\":time.clock()-start,\"no of entries\":0,\"list\":\"NA\"}  \n",
    "                   \n",
    "    '''If number of keys in the 1st dictionary is K it means that there are  itemsets of length K. \n",
    "    then if it exists it prints the itemsets else says that output doesn’t exist.'''\n",
    "                   \n",
    "    data=data.append(x,ignore_index=True)\n",
    "                   \n",
    "    with open('output.txt', 'w') as f: \n",
    "     for item in list(itemsets[0][K].keys()): \n",
    "        f.write(str(item)) \n",
    "        f.write(\"\\n\")\n",
    "    #saving the output in a txt file\n",
    "                   \n",
    "    print(\"The time taken is \",time.process_time()-start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time taken is  4.328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "itemset(\"kos\",4,15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bush', 'democratic', 'general', 'kerry')\n",
      "('bush', 'general', 'kerry', 'poll')\n",
      "('bush', 'general', 'kerry', 'war')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('output.txt', 'r') as f: \n",
    "    contents=f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
