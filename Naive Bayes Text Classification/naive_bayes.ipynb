{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library for extracting data out of HTML and XML type files. The Reuters data is a SGM type file, which is like HTML in terms of tages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[]\n",
    "for i in range(0,10):\n",
    "    files.append('reut2-00'+str(i)+'.sgm')\n",
    "for i in range(10,22):\n",
    "    files.append('reut2-0'+str(i)+'.sgm') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21 files, so we save the names of the files in a list such that we can extract them one after the another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['Topics','Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    \n",
    "    f = open(file, 'r')\n",
    "    data= f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(data)\n",
    "    \n",
    "    topics=soup.findAll('topics') \n",
    "    #Finds all occurrences of the tag \"topic\" and stores it as a list. \n",
    "    \n",
    "    y_topics=[]\n",
    "    for j in range(0,len(topics)):\n",
    "        x=[]\n",
    "        for i in topics[j]:\n",
    "            x.append(i.text)\n",
    "            '''i.text converts everything found between the tag as a string \n",
    "            and ignores all tags that come in between.'''\n",
    "        y_topics.append(x)\n",
    "    topics=y_topics  \n",
    "    '''The above loop and all is done because not all documents have topics.\n",
    "    So we don't want a mismatch between the document's index and its topics's.'''\n",
    "    \n",
    "    txt = soup.findAll('text')    \n",
    "    body=list()    \n",
    "    for i in txt:\n",
    "        body.append(str(i.text))\n",
    "    '''Extracting the body was easy as it was definitely there for all docs'''\n",
    "    \n",
    "    '''Now we consider only those docs in a data frame who have topics, \n",
    "    because otherwise we can't train our model.'''\n",
    "    for i in range(0,len(topics)):\n",
    "        \n",
    "        if len(topics[i])!=0:       \n",
    "            \n",
    "            body[i]=body[i].replace('\\d+',\" num \")       \n",
    "            '''replacing all the digits by num \n",
    "            as they are not really the ones defining our topics\n",
    "            and doing so makes our program run faster.'''\n",
    "            \n",
    "            z={'Topics':topics[i],'Body':body[i]}\n",
    "            df=df.append(z,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,jaccard_similarity_score,hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "x=df['Body']\n",
    "y=mlb.fit_transform(df['Topics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier does not allow the topics to be in list format, but rather expects them to be as a sparse matrix.\n",
    "fit_transform is used to convert multilabel to  a binary matrix that indicates the presence of class label. \n",
    "\n",
    "Thus the topics column is fitted and transformed using the multilabel binarizer. This transforms each item into a binary row \n",
    "of 0's and 1's and says if a topic is present(1), or not(0), in a particular item in the topics column.\n",
    "\n",
    "Multilabel binarizer is used as we can have multiple labels for the same document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.10, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into training and test, taking the test size to be 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LabelPowerset(ComplementNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we had to define a classifier.\n",
    "\n",
    "Label Powerset is a problem transformation approach to multi-label classification that transforms a multi-label problem to a multi-class problem with 1 multi-class classifier trained on all unique label combinations found in the training data, i.e. each combination is treated as a separate class and probability will be estimated per that class.\n",
    "\n",
    "I implemented the Complement Naive Bayes algorithm as it is suited for imbalanced data sets.\n",
    "Instead of calculating the likelihood of a word occurring in a class, it calculates the likelihood that the word occurs in other classes. A higher value means that it is highly likely that a document with those words does not belong to that class. \n",
    " \n",
    "Complement Naive Bayes outperforms Multinomial Naive Bayes on text classification tasks, as I also checked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('vectorizer', TfidfVectorizer(max_features=5000,analyzer=\"word\",stop_words=stopwords.words('english'))),\n",
    "                         ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline connects a series of steps into one object which you train and then use to make predictions, which leads to convenience in creating a easy to understand workflow. The purpose of the pipeline is to assemble several steps that can be crossvalidated together while setting different parameters. \n",
    "\n",
    "Here I've used a pipeline to connect the vectorization and classification tasks. \n",
    "\n",
    "TFIDF resolves the issue that a high frequency word might also be having a high frequency of occurrence in other documents as well. So TFIDF value is high only if a word has a high frequency of occurrence in that specific document but lower in all the other documents.\n",
    "\n",
    "Stop words are the less meaningful English words, which are supposed to be ignored while text classification.\n",
    "\n",
    "Note that TFIDF considers punctuations,etc., as separators and converts everything to lowercase by default. So we don't need to worry about the difference of words on the basis of case or presence of punctuation marks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...entNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False),\n",
       "       require_dense=[True, True]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is learning the vocabulary stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This predicts the view for the documents in our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation scores are: [0.78225176 0.7853792  0.79429018 0.77708252]\n",
      "The fraction of correctly classified samples according to \n",
      "Accuracy Score : 0.8082673702726473\n",
      "1-Hamming Loss : 0.996621225447083\n"
     ]
    }
   ],
   "source": [
    "print(\"The cross validation scores are: {}\".format(cross_val_score(pipeline, x_train, y_train, cv=4)))\n",
    "print(\"The fraction of correctly classified samples according to \\nAccuracy Score : {}\".format(accuracy_score(y_test, y_test_pred)))\n",
    "print(\"1-Hamming Loss : {}\".format(1-hamming_loss(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our general method of splitting the dataset into 2 parts for test and train has the disadvantage of our \n",
    "classifier not getting trained and validated on all examples in the data set, which is overcome by cross validation.\n",
    "The cross validation scores obtained as output should not vary too much for a good fit.\n",
    "\n",
    "Accuracy score computes the subset accuracy i.e. the set of labels predicted for a sample must \n",
    "exactly match the corresponding set of labels in y_true.\n",
    "\n",
    "Hamming loss is the fraction of labels that are incorrectly predicted.\n",
    "So (1-Hamming loss) is the fraction of correctly predicted labels. \n",
    "This penalizes the individual labels, unlike the accuracy_score which considers the entire set of labels \n",
    "for a given sample as incorrect if it does entirely match the true set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_actual=mlb.inverse_transform(y_test_pred)\n",
    "y_test_actual=mlb.inverse_transform(y_test)\n",
    "\n",
    "'''.fit_transform() -(Used before) It fits the label sets' binarizer and transforms the given label sets. \n",
    "   .inverse_transform -It tansforms the binary array to list of topics '''\n",
    "\n",
    "df_new=pd.DataFrame(columns=['Actual Labels','Predicted Labels','Document Body'])\n",
    "x_test=list(x_test)\n",
    "for j in range(0,len(y_test)):\n",
    "    df_new=df_new.append({'Actual Labels':y_test_actual[j],'Predicted Labels':y_test_pred_actual[j],'Document Body':x_test[j]},ignore_index=True)\n",
    "\n",
    "'''This is just to get the predicted and actual labels of our test data stored in an excel file.'''\n",
    "\n",
    "writer = pd.ExcelWriter('output_fitted.xlsx', engine='xlsxwriter') \n",
    "df_new.to_excel(writer, sheet_name='Sheet1') \n",
    "writer.save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
